{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maing class\n"
     ]
    }
   ],
   "source": [
    "### This file is a direct copy of the my_custom_dataset.py file in the models folder, but I've changed it to also return\n",
    "#    the label as well as the tensor. Could streamline this by editing that original file to do this if its testing but\n",
    "#    and not if its training, but being lazy for now and just making a whole new script.\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# for my transformations\n",
    "#import librosa\n",
    "from audiomentations import Compose, AddGaussianNoise, PitchShift, TimeStretch, ClippingDistortion, Gain, SevenBandParametricEQ\n",
    "\n",
    "\n",
    "def resize_mel_spectrogram(mel_spec, desired_shape=(224, 224)):\n",
    "    # Convert the 2D Mel spectrogram to 4D tensor (batch, channels, height, width)\n",
    "    mel_spec_tensor = torch.tensor(mel_spec).unsqueeze(0).unsqueeze(0)\n",
    "    # Resize\n",
    "    resized_mel_spec = F.interpolate(mel_spec_tensor, size=desired_shape, mode='bilinear', align_corners=False)\n",
    "    return resized_mel_spec.squeeze(0).squeeze(0).numpy()\n",
    "\n",
    "# augmentation\n",
    "augment_raw_audio = Compose(\n",
    "    [\n",
    "        AddGaussianNoise(min_amplitude=0.0001, max_amplitude=0.0005, p=1), # good\n",
    "        PitchShift(min_semitones=-2, max_semitones=12, p=0.5), #set values so it doesnt shift too low, rmeoving bomb signal\n",
    "        TimeStretch(p = 0.5), # defaults are fine\n",
    "        ClippingDistortion(0, 5, p = 0.5), # tested params to make sure its good\n",
    "        Gain(-10, 5, p = 0.5), # defaults are fine\n",
    "        # throws an error, so i commented it out\n",
    "        #SevenBandParametricEQ(-12, 12, p = 0.5)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Modify the load_audio_and_get_mel_spectrogram function:\n",
    "def mel_spectrogram_func(filename, augment, sr=8000, n_mels=128, n_fft=1024, hop_length=64, win_length=512):\n",
    "    y, _ = librosa.load(filename, sr=sr)\n",
    "\n",
    "    # apply transformations for train data if True, not for test data if False\n",
    "    if augment ==True:\n",
    "        # pass to augmentation function first then pass to mel spec below\n",
    "        audio_signal = augment_raw_audio(y, sr)\n",
    "    else:\n",
    "        # skip right to mel spec below\n",
    "        audio_signal = y\n",
    "\n",
    "    # compute the Mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio_signal, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n",
    "    mel_spectrogram_resized = resize_mel_spectrogram(mel_spectrogram)\n",
    "    return mel_spectrogram_resized\n",
    "\n",
    "\n",
    "\n",
    "class CTDataset_train(Dataset):\n",
    "    def __init__(self, cfg, split, transform, train_percent):\n",
    "        print('maing class')\n",
    "\n",
    "        '''\n",
    "            Constructor. Here, we collect and index the dataset inputs and labels.\n",
    "        '''\n",
    "        #if split == 'unlabeled':\n",
    "         #   print('This will not work unless you change the getitem function to have no labels for the unlabeled set') \n",
    "        self.data_root = cfg['data_path']\n",
    "        self.dataset = cfg['test_dataset']\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.train_percent = train_percent\n",
    "\n",
    "        # REMEMBER! there are 2 classes, so any changes should be made to both\n",
    "\n",
    "        # index data from JSON file\n",
    "        self.data = []\n",
    "        with open(cfg['json_path'], 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "            # Select only test_data and only from the right task (e.g australia)\n",
    "            filtered_audio = [entry for entry in json_data['audio'] if entry[\"data_type\"] == split and entry[\"dataset\"] == self.dataset]\n",
    "\n",
    "            # Now overwrite the 'audio' key in json_data with the filtered entries\n",
    "            json_data['audio'] = filtered_audio\n",
    "\n",
    "            # Now get all the classes in the filtered data json            \n",
    "            class_labels = set([obj['class'] for obj in json_data['audio']])\n",
    "\n",
    "            # Set a map that maps the class labels to integers\n",
    "            class_map = {class_label: i for i, class_label in enumerate(class_labels)}\n",
    "\n",
    "            # now append file paths and class label to self.data\n",
    "            for sublist in json_data.values():\n",
    "                for entry in sublist:\n",
    "                    path = entry[\"file_name\"]\n",
    "                    label = class_map[entry[\"class\"]]\n",
    "                    self.data.append([path, label]) \n",
    "\n",
    "            # Split data by classes\n",
    "            data_by_class = {}\n",
    "            for path, label in self.data:\n",
    "                if label not in data_by_class:\n",
    "                    data_by_class[label] = []\n",
    "                data_by_class[label].append([path, label])\n",
    "\n",
    "            # Sample according to train_percent and create training and testing lists\n",
    "            train_data = []\n",
    "            test_data = []\n",
    "            for label, entries in data_by_class.items():\n",
    "\n",
    "                # get length of the entries in this class, find train_precent * this len to be used in the split\n",
    "                n_train = int(len(entries) * train_percent)\n",
    "\n",
    "                # now do the split, we'll only take test data for this class\n",
    "                train_data.extend(entries[:n_train])\n",
    "                test_data.extend(entries[n_train:])\n",
    "\n",
    "        # shuffle data, fine as its already split\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(train_data) # uses seed set in train_eval.py\n",
    "        self.data = train_data\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "            Returns the length of the dataset.\n",
    "        '''\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "            Returns a single data point at given idx.\n",
    "            Here's where we actually load the audio and get the Mel spectrogram.\n",
    "        '''\n",
    "        #print(f'shape of id: {type(idx)}')\n",
    "        #print(idx)\n",
    "        audio_path, label = self.data[idx]\n",
    "\n",
    "        # load audio and get Mel spectrogram\n",
    "        if self.transform == True:\n",
    "            mel_spectrogram = mel_spectrogram_func(filename = os.path.join(self.data_root, audio_path), augment = True)\n",
    "        elif self.transform == False:\n",
    "            mel_spectrogram = mel_spectrogram_func(filename = os.path.join(self.data_root, audio_path), augment = False)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"The 'transform' parameter must be either True or False.\")\n",
    "\n",
    "            \n",
    "        \n",
    "        # make 3 dimensions, so shape goes from [x, y] to [3, x, y]\n",
    "        mel_spectrogram_tensor = torch.tensor(mel_spectrogram).unsqueeze(0).repeat(3, 1, 1).float()\n",
    "        \n",
    "        # the old transform call, its now ditched\n",
    "        #if self.transform:\n",
    "         #   mel_spectrogram_tensor = self.transform(mel_spectrogram_tensor)\n",
    "\n",
    "        # return the objects, label is commented out for now\n",
    "        return mel_spectrogram_tensor, label\n",
    "    \n",
    "\n",
    "# made a whole other class for the test dataloader cause am lazy. Could do something smarter.\n",
    "class CTDataset_test(Dataset):\n",
    "\n",
    "    def __init__(self, cfg, split, transform, train_percent):\n",
    "        '''\n",
    "            Constructor. Here, we collect and index the dataset inputs and labels.\n",
    "        '''\n",
    "        #if split == 'unlabeled':\n",
    "         #   print('This will not work unless you change the getitem function to have no labels for the unlabeled set') \n",
    "        self.data_root = cfg['data_path']\n",
    "        self.dataset = cfg['test_dataset']\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.train_percent = train_percent\n",
    "\n",
    "        # REMEMBER! there are 2 classes, so any changes should be made to both\n",
    "\n",
    "        # index data from JSON file\n",
    "        self.data = []\n",
    "        with open(cfg['json_path'], 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "            # Select only test_data and only from the right task (e.g australia)\n",
    "            filtered_audio = [entry for entry in json_data['audio'] if entry[\"data_type\"] == split and entry[\"dataset\"] == self.dataset]\n",
    "\n",
    "            # Now overwrite the 'audio' key in json_data with the filtered entries\n",
    "            json_data['audio'] = filtered_audio\n",
    "\n",
    "            # Now get all the classes in the filtered data json            \n",
    "            class_labels = set([obj['class'] for obj in json_data['audio']])\n",
    "\n",
    "            # Set a map that maps the class labels to integers\n",
    "            class_map = {class_label: i for i, class_label in enumerate(class_labels)}\n",
    "\n",
    "            # now append file paths and class label to self.data\n",
    "            for sublist in json_data.values():\n",
    "                for entry in sublist:\n",
    "                    path = entry[\"file_name\"]\n",
    "                    label = class_map[entry[\"class\"]]#entry[\"class\"]\n",
    "                    self.data.append([path, label]) ###chNGED TO LIST\n",
    "\n",
    "            # Split data by classes\n",
    "            data_by_class = {}\n",
    "            for path, label in self.data:\n",
    "                if label not in data_by_class:\n",
    "                    data_by_class[label] = []\n",
    "                data_by_class[label].append([path, label])\n",
    "\n",
    "            # Sample according to train_percent and create training and testing lists\n",
    "            train_data = []\n",
    "            test_data = []\n",
    "            for label, entries in data_by_class.items():\n",
    "\n",
    "                # get length of the entries in this class, find train_precent * this len to be used in the split\n",
    "                n_train = int(len(entries) * train_percent)\n",
    "\n",
    "                # now do the split, we'll only take test data for this class\n",
    "                train_data.extend(entries[:n_train])\n",
    "                test_data.extend(entries[n_train:])\n",
    "\n",
    "        # shuffle data, fine as its already split\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(test_data) # uses seed set in train_eval.py\n",
    "        self.data = test_data\n",
    "    def __len__(self):\n",
    "        '''\n",
    "            Returns the length of the dataset.\n",
    "        '''\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "            Returns a single data point at given idx.\n",
    "            Here's where we actually load the audio and get the Mel spectrogram.\n",
    "        '''\n",
    "\n",
    "        audio_path, label = self.data[idx]\n",
    "\n",
    "        # load audio and get Mel spectrogram\n",
    "        if self.transform == True:\n",
    "            mel_spectrogram = mel_spectrogram_func(filename = os.path.join(self.data_root, audio_path), augment = True)\n",
    "        elif self.transform == False:\n",
    "            mel_spectrogram = mel_spectrogram_func(filename = os.path.join(self.data_root, audio_path), augment = False)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"The 'transform' parameter must be either True or False.\")\n",
    "\n",
    "            \n",
    "        # make 3 dimensions, so shape goes from [x, y] to [3, x, y]\n",
    "        mel_spectrogram_tensor = torch.tensor(mel_spectrogram).unsqueeze(0).repeat(3, 1, 1).float()\n",
    "        \n",
    "        # the old transform call, its now ditched\n",
    "        #if self.transform:\n",
    "         #   mel_spectrogram_tensor = self.transform(mel_spectrogram_tensor)\n",
    "\n",
    "        # return the objects, label is commented out for now\n",
    "        return mel_spectrogram_tensor, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_sample = {\n",
    "    'data_path': '/mnt/ssd-cluster/ben/data/full_dataset/', \n",
    "    'json_path': '/home/ben/reef-audio-representation-learning/data/dataset.json',\n",
    "    'test_dataset': 'test_indonesia'\n",
    "}\n",
    "\n",
    "audio_dataset_test = CTDataset_test(cfg_sample, transform=False, train_percent=0.2, split='test_data')\n",
    "audio_dataset_train = CTDataset_test(cfg_sample, transform=False, train_percent=0.2, split='test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dataset_train = CTDataset_test(cfg_sample, transform=False, train_percent=0.2, split='test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.0297e-07, 1.1403e-06, 1.8965e-06,  ..., 1.6791e-06,\n",
       "          1.9489e-06, 1.2619e-06],\n",
       "         [2.4036e-07, 8.9778e-07, 1.7900e-06,  ..., 1.5223e-06,\n",
       "          1.9692e-06, 1.5178e-06],\n",
       "         [1.4019e-07, 5.0979e-07, 1.6195e-06,  ..., 1.2716e-06,\n",
       "          2.0018e-06, 1.9273e-06],\n",
       "         ...,\n",
       "         [3.5924e-07, 4.9018e-07, 3.7363e-07,  ..., 1.1159e-07,\n",
       "          8.1442e-08, 7.5090e-08],\n",
       "         [1.4601e-07, 1.9702e-07, 1.5114e-07,  ..., 5.1022e-08,\n",
       "          5.2935e-08, 5.9343e-08],\n",
       "         [1.2737e-08, 1.3805e-08, 1.2084e-08,  ..., 1.3170e-08,\n",
       "          3.5118e-08, 4.9501e-08]],\n",
       "\n",
       "        [[3.0297e-07, 1.1403e-06, 1.8965e-06,  ..., 1.6791e-06,\n",
       "          1.9489e-06, 1.2619e-06],\n",
       "         [2.4036e-07, 8.9778e-07, 1.7900e-06,  ..., 1.5223e-06,\n",
       "          1.9692e-06, 1.5178e-06],\n",
       "         [1.4019e-07, 5.0979e-07, 1.6195e-06,  ..., 1.2716e-06,\n",
       "          2.0018e-06, 1.9273e-06],\n",
       "         ...,\n",
       "         [3.5924e-07, 4.9018e-07, 3.7363e-07,  ..., 1.1159e-07,\n",
       "          8.1442e-08, 7.5090e-08],\n",
       "         [1.4601e-07, 1.9702e-07, 1.5114e-07,  ..., 5.1022e-08,\n",
       "          5.2935e-08, 5.9343e-08],\n",
       "         [1.2737e-08, 1.3805e-08, 1.2084e-08,  ..., 1.3170e-08,\n",
       "          3.5118e-08, 4.9501e-08]],\n",
       "\n",
       "        [[3.0297e-07, 1.1403e-06, 1.8965e-06,  ..., 1.6791e-06,\n",
       "          1.9489e-06, 1.2619e-06],\n",
       "         [2.4036e-07, 8.9778e-07, 1.7900e-06,  ..., 1.5223e-06,\n",
       "          1.9692e-06, 1.5178e-06],\n",
       "         [1.4019e-07, 5.0979e-07, 1.6195e-06,  ..., 1.2716e-06,\n",
       "          2.0018e-06, 1.9273e-06],\n",
       "         ...,\n",
       "         [3.5924e-07, 4.9018e-07, 3.7363e-07,  ..., 1.1159e-07,\n",
       "          8.1442e-08, 7.5090e-08],\n",
       "         [1.4601e-07, 1.9702e-07, 1.5114e-07,  ..., 5.1022e-08,\n",
       "          5.2935e-08, 5.9343e-08],\n",
       "         [1.2737e-08, 1.3805e-08, 1.2084e-08,  ..., 1.3170e-08,\n",
       "          3.5118e-08, 4.9501e-08]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_dataset_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.0297e-07, 1.1403e-06, 1.8965e-06,  ..., 1.6791e-06,\n",
       "          1.9489e-06, 1.2619e-06],\n",
       "         [2.4036e-07, 8.9778e-07, 1.7900e-06,  ..., 1.5223e-06,\n",
       "          1.9692e-06, 1.5178e-06],\n",
       "         [1.4019e-07, 5.0979e-07, 1.6195e-06,  ..., 1.2716e-06,\n",
       "          2.0018e-06, 1.9273e-06],\n",
       "         ...,\n",
       "         [3.5924e-07, 4.9018e-07, 3.7363e-07,  ..., 1.1159e-07,\n",
       "          8.1442e-08, 7.5090e-08],\n",
       "         [1.4601e-07, 1.9702e-07, 1.5114e-07,  ..., 5.1022e-08,\n",
       "          5.2935e-08, 5.9343e-08],\n",
       "         [1.2737e-08, 1.3805e-08, 1.2084e-08,  ..., 1.3170e-08,\n",
       "          3.5118e-08, 4.9501e-08]],\n",
       "\n",
       "        [[3.0297e-07, 1.1403e-06, 1.8965e-06,  ..., 1.6791e-06,\n",
       "          1.9489e-06, 1.2619e-06],\n",
       "         [2.4036e-07, 8.9778e-07, 1.7900e-06,  ..., 1.5223e-06,\n",
       "          1.9692e-06, 1.5178e-06],\n",
       "         [1.4019e-07, 5.0979e-07, 1.6195e-06,  ..., 1.2716e-06,\n",
       "          2.0018e-06, 1.9273e-06],\n",
       "         ...,\n",
       "         [3.5924e-07, 4.9018e-07, 3.7363e-07,  ..., 1.1159e-07,\n",
       "          8.1442e-08, 7.5090e-08],\n",
       "         [1.4601e-07, 1.9702e-07, 1.5114e-07,  ..., 5.1022e-08,\n",
       "          5.2935e-08, 5.9343e-08],\n",
       "         [1.2737e-08, 1.3805e-08, 1.2084e-08,  ..., 1.3170e-08,\n",
       "          3.5118e-08, 4.9501e-08]],\n",
       "\n",
       "        [[3.0297e-07, 1.1403e-06, 1.8965e-06,  ..., 1.6791e-06,\n",
       "          1.9489e-06, 1.2619e-06],\n",
       "         [2.4036e-07, 8.9778e-07, 1.7900e-06,  ..., 1.5223e-06,\n",
       "          1.9692e-06, 1.5178e-06],\n",
       "         [1.4019e-07, 5.0979e-07, 1.6195e-06,  ..., 1.2716e-06,\n",
       "          2.0018e-06, 1.9273e-06],\n",
       "         ...,\n",
       "         [3.5924e-07, 4.9018e-07, 3.7363e-07,  ..., 1.1159e-07,\n",
       "          8.1442e-08, 7.5090e-08],\n",
       "         [1.4601e-07, 1.9702e-07, 1.5114e-07,  ..., 5.1022e-08,\n",
       "          5.2935e-08, 5.9343e-08],\n",
       "         [1.2737e-08, 1.3805e-08, 1.2084e-08,  ..., 1.3170e-08,\n",
       "          3.5118e-08, 4.9501e-08]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_dataset_test[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1603"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio_dataset_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simclr_pytorch_reefs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
