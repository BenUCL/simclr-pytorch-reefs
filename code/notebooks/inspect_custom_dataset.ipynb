{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test custom dataset script works\n",
    "\n",
    "env = simclr_pytorch_reefs_new\n",
    "\n",
    "The first code block is the full script from custom dataset, after this there are a few checks to inspect this acts as it should e.g train length should be ~54k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    PyTorch dataset class for COCO-CT-formatted datasets. Note that you could\n",
    "    use the official PyTorch MS-COCO wrappers:\n",
    "    https://pytorch.org/vision/master/generated/torchvision.datasets.CocoDetection.html\n",
    "\n",
    "    We just hack our way through the COCO JSON files here for demonstration\n",
    "    purposes.\n",
    "\n",
    "    See also the MS-COCO format on the official Web page:\n",
    "    https://cocodataset.org/#format-data\n",
    "\n",
    "    2022 Benjamin Kellenberger\n",
    "'''\n",
    "\n",
    "\"\"\"\"\n",
    "    Turan adjusted this to work on his ROV data. Ben now adjusting to work on reef audio.\n",
    "    This will make a custom dataset.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "### turans old imports\n",
    "# import os\n",
    "# import json\n",
    "# from torch.utils.data import Dataset\n",
    "# from torchvision.transforms import Compose, Resize, ToTensor\n",
    "# from PIL import Image\n",
    "# import csv\n",
    "# from torchvision.transforms import transforms\n",
    "# from torchvision import transforms, datasets\n",
    "################\n",
    "\n",
    "\n",
    "# Preprocessor classes are used to load, transform, and augment audio samples for use in a machine learing model\n",
    "from opensoundscape.preprocess.preprocessors import SpectrogramPreprocessor\n",
    "from opensoundscape.ml.datasets import AudioFileDataset\n",
    "\n",
    "# helper function for displaying a sample as an image\n",
    "from opensoundscape.preprocess.utils import show_tensor, show_tensor_grid\n",
    "\n",
    "from opensoundscape import Action\n",
    "from opensoundscape.spectrogram import MelSpectrogram\n",
    "\n",
    "#other utilities and packages\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "import subprocess\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Imports by ben\n",
    "import json\n",
    "\n",
    "\n",
    "class CTDataset(Dataset):\n",
    "    \"\"\"\"\n",
    "    to us etry something like: (currently in ssl1)\n",
    "    cfg = {'dataset_path': '/home/ben/data/full_dataset/', #############################\n",
    "    'json_path': '/home/ben/data/dataset.json'}\n",
    "\n",
    "    #cfg = {'data_root':'/root/all_ROV_crops_with_unknown/all_ROV_crops_with_unknown', 'train_label_file':'../10_percent_train_with_unknown.csv', 'val_label_file':'../5_percent_val_with_unknown.csv', 'test_label_file':'../10_percent_test_with_unknown.csv', 'unlabeled_file':'../75_percent_unlabeled_with_unknown.csv'}\n",
    "    #### tarun : for pretraining self.trainset is the unlabeled dataset.\n",
    "    self.trainset = CTDataset(**cfg)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, json_path):   ########## make sure these get called in SSL1\n",
    "        # Pre-existing code to set seeds\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        random.seed(0)\n",
    "\n",
    "        # Load the JSON data from the file\n",
    "        with open(json_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "\n",
    "        # Extract the list of dictionaries from the \"audio\" key\n",
    "        audio_data = data.get(\"audio\", [])\n",
    "\n",
    "        # Filter the list to only include entries where data_type = \"train_data\"\n",
    "        self.data = [entry for entry in audio_data if entry.get(\"data_type\") == \"train_data\"]\n",
    "\n",
    "        # Convert the filtered list into a DataFrame\n",
    "        df = pd.DataFrame(self.data)\n",
    "\n",
    "        # Convert the list of dictionaries (which is the value of the main dictionary) into a DataFrame\n",
    "        #df = pd.DataFrame(data[list(data.keys())[0]])\n",
    "        #self.data = {k: v for k, v in data.items() if v.get(\"data_type\") == \"train_data\"}\n",
    "        #df = pd.DataFrame(self.data[list(self.data.keys())[0]])\n",
    "\n",
    "\n",
    "        # Create a dataframe with just file_path and a class column (req for AudioFileDataset)\n",
    "        transformed_df = df[['file_name', 'class']].copy()\n",
    "\n",
    "        # rename 'file_name' column to 'file'\n",
    "        transformed_df.rename(columns={'file_name': 'file'}, inplace=True)\n",
    "\n",
    "        # set file to be the index for AudioFileDataset\n",
    "        transformed_df.set_index('file', inplace=True)\n",
    "\n",
    "        # set all classes to 1 as AudioFileDataset requires class\n",
    "        transformed_df['class'] = 1\n",
    "\n",
    "        # append dataset_path to start of file_name column\n",
    "        transformed_df.index = dataset_path + transformed_df.index\n",
    "        #transformed_df.head() # for notebook\n",
    "\n",
    "        # initialize the preprocessor (forget what this does?)\n",
    "        pre = SpectrogramPreprocessor(sample_duration=1.92)\n",
    "\n",
    "        # initialize the dataset\n",
    "        self.dataset = AudioFileDataset(transformed_df, pre)\n",
    "\n",
    "        # change the bandpass from the default to 8kHz\n",
    "        self.dataset.preprocessor.pipeline.bandpass.set(min_f=0,max_f=8000)\n",
    "        \n",
    "        melspec_action = Action(self._my_melspec)\n",
    "        melspec_bandpass_action = Action(MelSpectrogram.bandpass, min_f=0, max_f=8000)\n",
    "\n",
    "        self.dataset.preprocessor.pipeline['to_spec'] = melspec_action\n",
    "        self.dataset.preprocessor.pipeline['bandpass'] = melspec_bandpass_action\n",
    "\n",
    "\n",
    "    ######## MOVE SOMEWHERE BETTER?\n",
    "    # custom functions to produce melspetrograms\n",
    "    def _melspec_linear_to_db(self, melspec):\n",
    "        \n",
    "        # because there's an underflow error during MelSpectrogram.from_audio() with dB_scale = True,\n",
    "        # we instead perform dB scaling afterwards\n",
    "        # which for some mysterious reason works\n",
    "        \n",
    "        melspectrogram = 10 * np.log10(\n",
    "                        melspec.spectrogram,\n",
    "                        where=melspec.spectrogram > 0,\n",
    "                        out=np.full(melspec.spectrogram.shape, -np.inf),)\n",
    "\n",
    "        # limit the decibel range (-100 to -20 dB by default)\n",
    "        # values below lower limit set to lower limit,\n",
    "        # values above upper limit set to uper limit\n",
    "        min_db, max_db = melspec.decibel_limits\n",
    "        melspectrogram[melspectrogram > max_db] = max_db\n",
    "        melspectrogram[melspectrogram < min_db] = min_db\n",
    "\n",
    "        return MelSpectrogram(times=melspec.times,\n",
    "                            frequencies=melspec.frequencies,\n",
    "                            spectrogram=melspectrogram,\n",
    "                            decibel_limits=melspec.decibel_limits,                   \n",
    "        )\n",
    "\n",
    "    def _my_melspec(self,audio):\n",
    "        melspec_linear = MelSpectrogram.from_audio(audio,dB_scale=False, window_samples = 512) #adjust params, use MelSpectrogram.from_audio to see what these are\n",
    "        melspec_db = self._melspec_linear_to_db(melspec_linear)\n",
    "        return melspec_db\n",
    "   \n",
    "   \n",
    "    def __len__(self):\n",
    "        # Adjust this if needed\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return the desired data point from self.dataset\n",
    "        return self.dataset[idx].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line will get passed in the \n",
    "cfg = {'dataset_path': '/home/ben/data/full_dataset/', #############################\n",
    "    'json_path': '/home/ben/data/dataset.json'}\n",
    "\n",
    "train_dataset = CTDataset(**cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54457"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  ..., -0.9536, -0.9941, -1.0015],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.9264, -0.9728, -0.9861],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.9311, -0.9720, -0.9760],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.8659, -0.8620, -0.8484],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ..., -0.9402, -0.9819, -1.0045],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.9479, -0.9679, -0.9936],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.9392, -0.9769, -0.9778],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.8786, -0.8593, -0.8494],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ..., -0.9700, -1.0057, -0.9784],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.9457, -0.9731, -0.9982],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.9418, -0.9639, -0.9801],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.8712, -0.8594, -0.8414],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m train_dataset\u001b[39m.\u001b[39mbypass_augmentations \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39m# change to True to prevent augmentations\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tensors \u001b[39m=\u001b[39m [train_dataset[i]\u001b[39m.\u001b[39mdata \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m)]\n\u001b[0;32m----> 6\u001b[0m sample_labels \u001b[39m=\u001b[39m [\u001b[39mlist\u001b[39m(train_dataset[i]\u001b[39m.\u001b[39mlabels[train_dataset[i]\u001b[39m.\u001b[39mlabels\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mindex) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m)]\n\u001b[1;32m      8\u001b[0m _ \u001b[39m=\u001b[39m show_tensor_grid(tensors,\u001b[39m3\u001b[39m,labels\u001b[39m=\u001b[39msample_labels)\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m train_dataset\u001b[39m.\u001b[39mbypass_augmentations \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39m# change to True to prevent augmentations\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tensors \u001b[39m=\u001b[39m [train_dataset[i]\u001b[39m.\u001b[39mdata \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m)]\n\u001b[0;32m----> 6\u001b[0m sample_labels \u001b[39m=\u001b[39m [\u001b[39mlist\u001b[39m(train_dataset[i]\u001b[39m.\u001b[39;49mlabels[train_dataset[i]\u001b[39m.\u001b[39mlabels\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mindex) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m)]\n\u001b[1;32m      8\u001b[0m _ \u001b[39m=\u001b[39m show_tensor_grid(tensors,\u001b[39m3\u001b[39m,labels\u001b[39m=\u001b[39msample_labels)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'labels'"
     ]
    }
   ],
   "source": [
    "# sample.data returns the tensor\n",
    "# sample.label returns the label (all 1)\n",
    "\n",
    "train_dataset.bypass_augmentations = False # change to True to prevent augmentations\n",
    "tensors = [train_dataset[i].data for i in range(3)]\n",
    "sample_labels = [list(train_dataset[i].labels[train_dataset[i].labels>0].index) for i in range(3)]\n",
    "\n",
    "_ = show_tensor_grid(tensors,3,labels=sample_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simclr_pytorch_reefs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
